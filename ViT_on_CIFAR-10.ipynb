{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "7ZVgZKwPV8Z8",
        "VQk7oz0eovZb",
        "Kx3SZutpeve5",
        "WBmDMoNSmERg",
        "AnuEwB5SN9Jj",
        "Tt_clkp8Ytna",
        "Qbdb7aFVe_ze"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## implementing ViT from scratch\n",
        "Training a vision transformer model on CIFAR-10 dataset"
      ],
      "metadata": {
        "id": "cYadN4wqn758"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "YPvuJ2MJnvou"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import torchvision\n",
        "import numpy as np\n",
        "import torch.nn as nn"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Steps:\n",
        "\n",
        "\n",
        "*   prepare data\n",
        "*   prepare patch embeddings from image input\n",
        "*   add positional embeddings\n",
        "*   attention head\n",
        "*   multiple attention heads\n",
        "*   feed forward network in transformer block\n",
        "*   transformer block\n",
        "*   classification head\n",
        "*   training\n",
        "\n"
      ],
      "metadata": {
        "id": "-x1olrRuoJHk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data preparation"
      ],
      "metadata": {
        "id": "7ZVgZKwPV8Z8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 16"
      ],
      "metadata": {
        "id": "VqzYkuZ8dPnk"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transformations = torchvision.transforms.Compose([\n",
        "    torchvision.transforms.ToTensor(),        # converts to tensor and brings pixel values in the range (0,1)\n",
        "    torchvision.transforms.Resize((32,32)),\n",
        "    torchvision.transforms.Normalize((0.5,0.5,0.5), (0.5,0.5,0.5))       # brings the values in the range (-1,1) which is desirable for computations in the network\n",
        "])\n",
        "\n",
        "train_set = torchvision.datasets.CIFAR10(root=\".\", train=True, transform=transformations, download=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jMBmDj4fV91F",
        "outputId": "b47add5a-d66d-4d8f-b699-6011311ae778"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = torch.utils.data.DataLoader(train_set, batch_size = BATCH_SIZE, shuffle=True)"
      ],
      "metadata": {
        "id": "v4-KCNh1dAlb"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_set = torchvision.datasets.CIFAR10(root=\".\", train=False, transform=transformations, download=True)\n",
        "len(test_set)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kfFoXcKBdYR3",
        "outputId": "9cc391b9-b785-4169-b608-eb7850560828"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10000"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_loader = torch.utils.data.DataLoader(test_set, batch_size = BATCH_SIZE, shuffle=False)"
      ],
      "metadata": {
        "id": "Q1HX2NLTdlnI"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Embeddings"
      ],
      "metadata": {
        "id": "VQk7oz0eovZb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "input image is broken into patches and patch embeddings are made.\n",
        "This can be implemented using a conv2D layer of kernel size equal to the patch size, stride is also the same.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "rdUzVptgoFWf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PatchEmbeddings(nn.Module):\n",
        "  def __init__(self, image_channels, patch_size, embedding_size):\n",
        "    super().__init__()\n",
        "    self.image_channels = image_channels\n",
        "    self.patch_size = patch_size\n",
        "    self.embedding_size = embedding_size\n",
        "    self.conv = nn.Conv2d(in_channels=self.image_channels,\n",
        "                          out_channels=self.embedding_size,\n",
        "                          kernel_size=self.patch_size,\n",
        "                          stride=self.patch_size)\n",
        "\n",
        "  def __call__(self, x):\n",
        "    # x shape --> (B, C, H, H)\n",
        "    x = self.conv(x)                                  # (B, embedding_size, patches_x, patches_y)\n",
        "    x = x.flatten(start_dim=2, end_dim=-1)            # (B, embedding_size, num_patches)\n",
        "    x = x.transpose(-1, -2)                           # (B, num_patches, embedding_size)\n",
        "    return x"
      ],
      "metadata": {
        "id": "UVOymFL6pSK4"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# add positional embeddings and cls token\n",
        "# positional embeddings will be added in the form of an Embedding layer\n",
        "\n",
        "class FinalEmbeddings(nn.Module):\n",
        "  def __init__(self, image_channels, patch_size, num_patches, embedding_size):\n",
        "    super().__init__()\n",
        "    self.patch_embeddings = PatchEmbeddings(image_channels, patch_size, embedding_size)\n",
        "    self.num_patches = num_patches\n",
        "    self.embedding_size = embedding_size\n",
        "    self.positional_embedding = nn.Embedding(num_embeddings = self.num_patches + 1,\n",
        "                                             embedding_dim = self.embedding_size,\n",
        "                                             dtype = torch.float32)\n",
        "    self.cls_embedding = nn.Parameter(torch.randn(1, 1, self.embedding_size))\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.patch_embeddings(x)\n",
        "    patch_positional_embeddings = self.positional_embedding(torch.arange(1, self.num_patches + 1))  # (num_patches, embedding_size)\n",
        "    cls_positional_embedding = self.positional_embedding(torch.tensor([0]))                  # (embedding_size)\n",
        "\n",
        "    # reshaping\n",
        "    cls_positional_embedding = cls_positional_embedding.view((1, self.embedding_size))\n",
        "    positional_embeddings = torch.cat([cls_positional_embedding, patch_positional_embeddings], dim=0)   # (num_patches + 1, embedding_size)\n",
        "    positional_embeddings = positional_embeddings.view((1, self.num_patches + 1, self.embedding_size))\n",
        "\n",
        "    batch_size = x.size(0)\n",
        "    cls_embeddings = self.cls_embedding.expand(batch_size, 1, -1)       # expanding cls token to all examples of batch\n",
        "\n",
        "    # concatenate input patch embeddings and cls_embedding\n",
        "    x = torch.cat([cls_embeddings, x], dim=1)          # (B, num_patches + 1, embedding_size)\n",
        "\n",
        "    # adding positional embeddings and embeddings\n",
        "    x = x + positional_embeddings\n",
        "    return x"
      ],
      "metadata": {
        "id": "SNlu85nRgVHz"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Multi Head attention block"
      ],
      "metadata": {
        "id": "Kx3SZutpeve5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# a single attention head\n",
        "\n",
        "class AttentionHead(nn.Module):\n",
        "  def __init__(self, embedding_size, head_size):\n",
        "    super().__init__()\n",
        "    self.head_size = head_size\n",
        "    self.embedding_size = embedding_size\n",
        "    self.query = nn.Linear(self.embedding_size, self.head_size, bias=True)\n",
        "    self.key = nn.Linear(self.embedding_size, self.head_size, bias=True)\n",
        "    self.value = nn.Linear(self.embedding_size, self.head_size, bias=True)\n",
        "\n",
        "  def forward(self, x):\n",
        "    # key, query, value tensors are projected from the input tensor x\n",
        "    query = self.query(x)       # (B, num_patches + 1, head_size)\n",
        "    key = self.key(x)\n",
        "    value = self.value(x)\n",
        "    # scaled dot product attention weights\n",
        "    weights = (key @ query.transpose(-1, -2)) / np.sqrt(self.head_size)   # (B, num_patches + 1, num_patches + 1)\n",
        "    weights = nn.functional.softmax(weights, dim=-1)\n",
        "    logits = weights @ value                 # (B, num_patches + 1, num_patches + 1) @ (B, num_patches + 1, head_size) ---> (B, num_patches + 1, head_size)\n",
        "    return logits"
      ],
      "metadata": {
        "id": "cTmdwy7rWWQS"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# multihead attention block\n",
        "\n",
        "class MultiHeadAttn(nn.Module):\n",
        "  def __init__(self, num_heads, embedding_size):\n",
        "    super().__init__()\n",
        "    self.num_heads = num_heads\n",
        "    self.embedding_size = embedding_size\n",
        "    self.head_size = self.embedding_size//self.num_heads\n",
        "    self.all_head_size = self.head_size * self.num_heads\n",
        "    self.ma_head = nn.ModuleList([AttentionHead(self.embedding_size, self.head_size) for _ in range(self.num_heads)])\n",
        "    self.projection = nn.Linear(self.all_head_size, self.embedding_size)\n",
        "\n",
        "  def forward(self, x):\n",
        "    # x --> (B, num_patches + 1, embedding_size)\n",
        "    attn_output = torch.cat([head(x) for head in self.ma_head], dim=-1)    # (B, num_patches + 1, all_head_size)\n",
        "    attn_output = self.projection(attn_output)                             # (B, num_patches + 1, embedding_size)\n",
        "    return attn_output"
      ],
      "metadata": {
        "id": "3GNaGWgOkIYv"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Feed forward network in transformer block"
      ],
      "metadata": {
        "id": "WBmDMoNSmERg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# a simple 2-layer MLP\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "  def __init__(self, embedding_size, intermediate_size):\n",
        "    super().__init__()\n",
        "    self.embedding_size = embedding_size\n",
        "    self.intermediate_size = intermediate_size\n",
        "    self.layer1 = nn.Linear(self.embedding_size, self.intermediate_size)\n",
        "    self.activation = nn.LeakyReLU()\n",
        "    self.layer2 = nn.Linear(self.intermediate_size, self.embedding_size)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.layer1(x)      # (B, num_patches + 1, intermediate_size)\n",
        "    x = self.activation(x)\n",
        "    x = self.layer2(x)      # (B, num_patches + 1, embedding_size)\n",
        "    return x"
      ],
      "metadata": {
        "id": "ubwU0vbcmB4h"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transformer block"
      ],
      "metadata": {
        "id": "AnuEwB5SN9Jj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# a single transformer block\n",
        "\"\"\"\n",
        "a single block will contain:\n",
        "1. multi head attention layer\n",
        "2. feed forward network\n",
        "3. layer normalization and skip connection before and after the multi head attention block\n",
        "\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "F3nsW6HMN-lS",
        "outputId": "02c1261d-1d49-4596-918a-357c38c0b182"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\na single block will contain:\\n1. multi head attention layer\\n2. feed forward network\\n3. layer normalization and skip connection before and after the multi head attention block\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerBlock(nn.Module):\n",
        "  def __init__(self, num_heads, embedding_size, intermediate_size):\n",
        "    super().__init__()\n",
        "    self.num_heads = num_heads\n",
        "    self.embedding_size = embedding_size\n",
        "    self.intermediate_size = intermediate_size\n",
        "\n",
        "    self.layernorm1 = nn.LayerNorm(self.embedding_size)\n",
        "    self.mha = MultiHeadAttn(self.num_heads, self.embedding_size)\n",
        "    self.layernorm2 = nn.LayerNorm(self.embedding_size)\n",
        "    self.feed_forward = FeedForward(self.embedding_size, self.intermediate_size)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.layernorm1(x)\n",
        "    x = x + self.mha(x)            # (skip connection)\n",
        "    x = self.layernorm2(x)\n",
        "    x = x + self.feed_forward(x)   # (skip connection)\n",
        "    return x"
      ],
      "metadata": {
        "id": "YUTJqTVUOjtA"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Encoder"
      ],
      "metadata": {
        "id": "Tt_clkp8Ytna"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "  def __init__(self, num_layers, num_heads, embedding_size, intermediate_size):\n",
        "    super().__init__()\n",
        "    # Create a list of transformer blocks\n",
        "    self.num_layers = num_layers\n",
        "    self.num_heads = num_heads\n",
        "    self.embedding_size = embedding_size\n",
        "    self.intermediate_size = intermediate_size\n",
        "    self.blocks = nn.ModuleList([TransformerBlock(self.num_heads,\n",
        "                                                  self.embedding_size,\n",
        "                                                  self.intermediate_size) for _ in range(self.num_layers)])\n",
        "\n",
        "  def forward(self, x):\n",
        "    for block in self.blocks:\n",
        "      x = block(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "Fu2ZX5KfWjRe"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model"
      ],
      "metadata": {
        "id": "Qbdb7aFVe_ze"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleVIT(nn.Module):\n",
        "  def __init__(self, num_classes, image_channels, patch_size, num_patches, embedding_size, intermediate_size, num_layers, num_heads):\n",
        "    super().__init__()\n",
        "    self.num_classes = num_classes\n",
        "    self.image_channels = image_channels\n",
        "    self.patch_size = patch_size\n",
        "    self.num_patches = num_patches\n",
        "    self.embedding_size = embedding_size\n",
        "    self.intermediate_size = intermediate_size\n",
        "    self.num_layers = num_layers\n",
        "    self.num_heads = num_heads\n",
        "    self.embeddings = FinalEmbeddings(self.image_channels,\n",
        "                                      self.patch_size,\n",
        "                                      self.num_patches,\n",
        "                                      self.embedding_size)\n",
        "    self.encoder = Encoder(self.num_layers, self.num_heads, self.embedding_size, self.intermediate_size)\n",
        "    self.classifier = nn.Linear(self.embedding_size, self.num_classes)\n",
        "\n",
        "  def forward(self, x):\n",
        "    embeddings = self.embeddings(x)          # (B,C,H,W) --> (B, num_patches+1, embedding_size)\n",
        "    encoder_output = self.encoder(embeddings)   # (B, num_patches+1, embedding_size)\n",
        "\n",
        "    # taking the embedding corresponding to CLS token for classification\n",
        "    logits = self.classifier(encoder_output[:,0,:])    # (B, 1, num_classes)\n",
        "    return logits\n"
      ],
      "metadata": {
        "id": "8OZ-aC6hfA_7"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training"
      ],
      "metadata": {
        "id": "7QH7RC8SVag8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 10\n",
        "learning_rate = 0.001\n",
        "num_classes = 10\n",
        "image_channels = 3\n",
        "patch_size = 4\n",
        "image_dim = 32\n",
        "num_patches = (image_dim // patch_size)**2\n",
        "embedding_size = 48\n",
        "intermediate_size = 4*embedding_size\n",
        "num_layers = 4\n",
        "num_heads = 4\n",
        "\n",
        "model = SimpleVIT(num_classes, image_channels, patch_size, num_patches, embedding_size,\n",
        "                  intermediate_size, num_layers, num_heads)\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "loss_fn = nn.CrossEntropyLoss()"
      ],
      "metadata": {
        "id": "Wzm_UD2izzep"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_losses, test_losses, accuracies = [], [], []\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "  for batch in train_loader:\n",
        "    x, y = batch[0], batch[1]\n",
        "    optimizer.zero_grad()\n",
        "    output_logits = model(x)\n",
        "    loss = loss_fn(output_logits, y)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    train_losses.append(loss.item())"
      ],
      "metadata": {
        "id": "E_CEBEFTd6RJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}